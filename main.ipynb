{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 3 Anomaly Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection\n",
    "Training a model to determine whether the given image is similar with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T09:47:51.110640Z",
     "iopub.status.busy": "2023-11-13T09:47:51.110329Z",
     "iopub.status.idle": "2023-11-13T09:47:53.810289Z",
     "shell.execute_reply": "2023-11-13T09:47:53.809348Z",
     "shell.execute_reply.started": "2023-11-13T09:47:51.110613Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## # Training progress bar\n",
    "!pip install -q qqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T09:47:53.811874Z",
     "iopub.status.busy": "2023-11-13T09:47:53.811477Z",
     "iopub.status.idle": "2023-11-13T09:47:55.896282Z",
     "shell.execute_reply": "2023-11-13T09:47:55.895462Z",
     "shell.execute_reply.started": "2023-11-13T09:47:53.811842Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\r\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\r\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import MutableMapping\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import Iterable, Mapping\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n",
      "  from collections import Sized\r\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.io import Dataset, DataLoader\n",
    "import paddle.vision.transforms as transforms\n",
    "from paddle.optimizer import Adam, AdamW\n",
    "import paddle.nn.functional as F\n",
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from qqdm import qqdm, format_str\n",
    "import pandas as pd\n",
    "# FOr eval\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T09:47:55.898093Z",
     "iopub.status.busy": "2023-11-13T09:47:55.897381Z",
     "iopub.status.idle": "2023-11-13T09:47:55.901506Z",
     "shell.execute_reply": "2023-11-13T09:47:55.900834Z",
     "shell.execute_reply.started": "2023-11-13T09:47:55.898061Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip -o data/data173899/pr_2023_fall_a3.zip -d work/\n",
    "#!ls work/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclude:\n",
    "* trainingset.npy: training data, 90000 human faces\n",
    "* testingset.npy: testing data\n",
    "    * About 5000 from the same distribution with training data (label 0)\n",
    "    * About 5000 from another distribution (anomalies, label 1)\n",
    "* privatetestingset.npy: private testing data\n",
    "* test_y.npy \n",
    "    * nomal label: 0。anomal label: 1.\n",
    "\n",
    "Shape: (#images, 64, 64, 3) for each .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-13T09:47:55.902849Z",
     "iopub.status.busy": "2023-11-13T09:47:55.902428Z",
     "iopub.status.idle": "2023-11-13T09:47:57.448246Z",
     "shell.execute_reply": "2023-11-13T09:47:57.447461Z",
     "shell.execute_reply.started": "2023-11-13T09:47:55.902824Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train dataset is (90000, 64, 64, 3). The shape of test dataset is (10000, 64, 64, 3).\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_root='work/'\n",
    "\n",
    "train = np.load(data_root + 'trainingset.npy', allow_pickle=True)\n",
    "test = np.load(data_root + 'testingset.npy', allow_pickle=True)\n",
    "test_y = np.load(data_root + 'test_y.npy', allow_pickle=True)\n",
    "\n",
    "print(f'The shape of train dataset is {train.shape}. The shape of test dataset is {test.shape}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T19:37:50.292772Z",
     "iopub.status.busy": "2022-10-24T19:37:50.291980Z",
     "iopub.status.idle": "2022-10-24T19:37:50.298696Z",
     "shell.execute_reply": "2022-10-24T19:37:50.297599Z",
     "shell.execute_reply.started": "2022-10-24T19:37:50.292713Z"
    }
   },
   "source": [
    "## Model and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auto encoder is a basic generation model. It uses the encoder-decoder architecture to encode first (such as compressing the image into a lower-dimensional vector), then decode (such as restoring the low-dimensional vector to an image), and restore the image.We use reconstruction loss to measure the model. The closer it is to the original image, the better, reconstruction error smaller. The common transformer model is this auto-encoder model.This work implements ' fcn_autoencoder', 'conv_autoencoder'and 'VAE'.\n",
    "* fcn_autoencoder and vae are from https://github.com/L1aoXingyu/pytorch-beginner   \n",
    "* conv_autoencoder is from https://github.com/jellycsc/PyTorch-CIFAR-10-autoencoder/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T09:47:57.450361Z",
     "iopub.status.busy": "2023-11-13T09:47:57.449669Z",
     "iopub.status.idle": "2023-11-13T09:47:57.475990Z",
     "shell.execute_reply": "2023-11-13T09:47:57.475284Z",
     "shell.execute_reply.started": "2023-11-13T09:47:57.450331Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class fcn_autoencoder(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(fcn_autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64* 64 * 3, 128),  # 3072→128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, 12), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12, 3)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 64 * 64 * 3), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class conv_autoencoder(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(conv_autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2D(3, 64, 4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "            nn.Conv2D(64, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "\t\t\tnn.Conv2D(64, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(128),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2DTranspose(128, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "\t\t\tnn.Conv2DTranspose(64, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "            nn.Conv2DTranspose(64, 3, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(3),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class conv_autoencoder_classifier(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(conv_autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2D(3, 64, 4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "            nn.Conv2D(64, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "\t\t\tnn.Conv2D(64, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(128),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2DTranspose(128, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "\t\t\tnn.Conv2DTranspose(64, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "            nn.Conv2DTranspose(64, 3, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(3),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class VAE(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2D(3, 12, 4, stride=2, padding=1),            \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(12, 24, 4, stride=2, padding=1),    \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.enc_out_1 = nn.Sequential(\n",
    "            nn.Conv2D(24, 48, 4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.enc_out_2 = nn.Sequential(\n",
    "            nn.Conv2D(24, 48, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "\t\t\tnn.Conv2DTranspose(48, 24, 4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "\t\t    nn.Conv2DTranspose(24, 12, 4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2DTranspose(12, 3, 4, stride=2, padding=1), \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        return self.enc_out_1(h1), self.enc_out_2(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        # std = logvar.mul(0.5).exp_()  #文档错误\n",
    "        std = logvar.multiply(paddle.to_tensor([0.5])).exp()\n",
    "        # eps = paddle.to_tensor(std.shape()).normal_()  #错误\n",
    "        eps = paddle.tensor.standard_normal(std.shape)\n",
    "        # eps = Variable(eps)  #感觉没必要\n",
    "        # return eps.mul(std).add_(mu)  #错误\n",
    "        return eps.multiply(std).add(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "        \n",
    "class MultiEncoderAutoencoder(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MultiEncoderAutoencoder, self).__init__()\n",
    "        \n",
    "        # Define the first encoder path\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2D(3, 64, 4, stride=2, padding=1), \n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm2D(64),\n",
    "            nn.Conv2D(64, 64, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.BatchNorm2D(64),\n",
    "\t\t\tnn.Conv2D(64, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(128),\n",
    "        )\n",
    "        \n",
    "        # Define the second encoder path\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2D(3, 64, 4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "            nn.Conv2D(64, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "\t\t\tnn.Conv2D(64, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(128),\n",
    "        )\n",
    "        \n",
    "        # Define the decoder path\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2DTranspose(128, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "\t\t\tnn.Conv2DTranspose(64, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(64),\n",
    "            nn.Conv2DTranspose(64, 3, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2D(3),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, add_noise=False, noise_factor=0.1):\n",
    "        if add_noise:\n",
    "            noise = paddle.randn(x.shape) * noise_factor\n",
    "            x = x + noise\n",
    "        # Pass input through the first encoder\n",
    "        encoded1 = self.encoder1(x)\n",
    "        # Pass input through the second encoder\n",
    "        encoded2 = self.encoder2(x)\n",
    "        # Combine the outputs of both encoders\n",
    "        encoded = paddle.concat((encoded1, encoded2), axis=0)\n",
    "        # Pass the combined encoded output through the decoder\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def loss_vae(recon_x, x, mu, logvar, criterion):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    mse = criterion(recon_x, x)  # mse loss\n",
    "\n",
    "    KLD_element = mu.pow(2).add(logvar.exp()).multiply(paddle.to_tensor([-1.0])).add(paddle.to_tensor([1.0])).add(logvar)\n",
    "    KLD = paddle.sum(KLD_element).multiply(paddle.to_tensor([-0.5]))\n",
    "\n",
    "    # KL divergence\n",
    "    return mse + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module for obtaining and processing data. The transform function here normalizes image's pixels from [0, 255] to [-1.0, 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-13T09:47:57.477382Z",
     "iopub.status.busy": "2023-11-13T09:47:57.476971Z",
     "iopub.status.idle": "2023-11-13T09:47:57.484370Z",
     "shell.execute_reply": "2023-11-13T09:47:57.483690Z",
     "shell.execute_reply.started": "2023-11-13T09:47:57.477356Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = paddle.to_tensor(tensors,dtype=\"float32\")\n",
    "        if tensors.shape[-1] == 3:\n",
    "            self.tensors = paddle.transpose(self.tensors, (0, 3, 1, 2))\n",
    "\n",
    "        self.tensors = self.tensors * 2. / 255.  -1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[index]\n",
    "       \n",
    "        return x, paddle.to_tensor([-1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-13T09:56:31.267730Z",
     "iopub.status.busy": "2023-11-13T09:56:31.265467Z",
     "iopub.status.idle": "2023-11-13T09:57:10.943799Z",
     "shell.execute_reply": "2023-11-13T09:57:10.942707Z",
     "shell.execute_reply.started": "2023-11-13T09:56:31.267684Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:142: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\r\n",
      "  data = np.array(data)\r\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n\tFaild to convert input data to a regular ndarray :\n\t - Usually this means the input data contains nested lists with different lengths. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8633/2979379296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Build training dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8633/2122714578.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCustomTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-234>\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(data, dtype, place, stop_gradient)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    224\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(data, dtype, place, stop_gradient)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0;34m\"\\n\\tFaild to convert input data to a regular ndarray :\\n\\t - Usually \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                     \u001b[0;34m\"this means the input data contains nested lists with different lengths. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: \n\tFaild to convert input data to a regular ndarray :\n\t - Usually this means the input data contains nested lists with different lengths. "
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "\n",
    "batch_size = 2000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Build training dataloader\n",
    "train_dataset = CustomTensorDataset(train)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Model\n",
    "model_type = 'cnn' # selecting a model type from {'cnn', 'fcn', 'vae'}\n",
    "model_classes = {'fcn':fcn_autoencoder(), 'cnn':conv_autoencoder(), 'vae':VAE()}\n",
    "model = model_classes[model_type]\n",
    "#model = MultiEncoderAutoencoder()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(parameters=model.parameters(), learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-13T09:48:03.663059Z",
     "iopub.status.idle": "2023-11-13T09:48:03.663672Z",
     "shell.execute_reply": "2023-11-13T09:48:03.663504Z",
     "shell.execute_reply.started": "2023-11-13T09:48:03.663480Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#开始训练\n",
    "best_loss = np.inf\n",
    "model.train()\n",
    "\n",
    "qqdm_train = qqdm(range(num_epochs), desc=format_str('bold', 'Description'))\n",
    "for epoch in qqdm_train:\n",
    "    tot_loss = list()\n",
    "    for data in train_dataloader():\n",
    "\n",
    "        # ===================loading=====================\n",
    "        img = data[0]\n",
    "        if model_type in ['fcn']:\n",
    "            img = paddle.reshape(img, [img.shape[0], -1])\n",
    "\n",
    "        # ===================forward=====================        \n",
    "        output = model(img)\n",
    "        \n",
    "        if model_type == 'vae':\n",
    "            loss = loss_vae(output[0], img, output[1], output[2], criterion)\n",
    "        else:\n",
    "            loss = criterion(output, img)\n",
    "            closs += nn.BCELoss()(classification, labels)\n",
    "        \n",
    "        tot_loss.append(loss.numpy())\n",
    "        # ===================backward====================\n",
    "        optimizer.clear_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================save_best====================\n",
    "    mean_loss = np.mean(tot_loss)\n",
    "    if mean_loss < best_loss:\n",
    "        best_loss = mean_loss\n",
    "        paddle.save(model.state_dict(), 'best_model_{}.pdparams'.format(model_type))\n",
    "    # ===================log========================\n",
    "    qqdm_train.set_infos({\n",
    "        'epoch': f'{epoch + 1:.0f}/{num_epochs:.0f}',\n",
    "        'loss': f'{mean_loss:.4f}',\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "After inputting the testing image into the model, the reconstructed image can be obtained, and the squared difference between images can be obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T09:48:03.664914Z",
     "iopub.status.idle": "2023-11-13T09:48:03.665515Z",
     "shell.execute_reply": "2023-11-13T09:48:03.665358Z",
     "shell.execute_reply.started": "2023-11-13T09:48:03.665342Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 开始预测\n",
    "eval_batch_size = 200\n",
    "\n",
    "# build testing dataloader\n",
    "test_dataset = CustomTensorDataset(test)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "eval_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "# load trained model\n",
    "layer_state_dict = paddle.load('best_model_{}.pdparams'.format(model_type))\n",
    "model.set_state_dict(layer_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2023-11-13T09:48:03.667371Z",
     "iopub.status.idle": "2023-11-13T09:48:03.667677Z",
     "shell.execute_reply": "2023-11-13T09:48:03.667542Z",
     "shell.execute_reply.started": "2023-11-13T09:48:03.667529Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomality = list()\n",
    "for i, data in enumerate(test_dataloader()): \n",
    "    img = data[0]\n",
    "    if model_type == 'fcn':\n",
    "        img = paddle.reshape(img, [img.shape[0], -1])\n",
    "    output = model(img)\n",
    "    if model_type == 'vae':\n",
    "        output = output[0]\n",
    "    if model_type =='fcn':\n",
    "        loss = eval_loss(output, img).sum(-1)\n",
    "    else:\n",
    "        loss = eval_loss(output, img).sum([1,2,3])\n",
    "    anomality.append(loss.numpy())\n",
    "anomality = np.concatenate(anomality, axis=0)\n",
    "anomality = np.sqrt(anomality).reshape(len(test),1)\n",
    "\n",
    "roc_score = roc_auc_score(test_y, anomality)\n",
    "print(\"test roc_scoreacc is: {}\".format(roc_score))\n",
    "print(\"best loss is {}\".format(best_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T09:48:03.669682Z",
     "iopub.status.idle": "2023-11-13T09:48:03.670253Z",
     "shell.execute_reply": "2023-11-13T09:48:03.670091Z",
     "shell.execute_reply.started": "2023-11-13T09:48:03.670075Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 展示异常的图片\n",
    "showlist = anomality.reshape(-1).argsort()[-5:].tolist()\n",
    "for i,ind in enumerate(showlist):\n",
    "    pic  = test[ind]\n",
    "    plt.subplot(1,len(showlist),i+1)\n",
    "    plt.imshow(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T09:48:03.671805Z",
     "iopub.status.idle": "2023-11-13T09:48:03.672129Z",
     "shell.execute_reply": "2023-11-13T09:48:03.671986Z",
     "shell.execute_reply.started": "2023-11-13T09:48:03.671973Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 展示正常的图片\n",
    "showlist = anomality.reshape(-1).argsort()[:5].tolist()\n",
    "for i,ind in enumerate(showlist):\n",
    "    pic  = test[ind]\n",
    "    plt.subplot(1,len(showlist),i+1)\n",
    "    plt.imshow(pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Private Test Set Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'prediction.csv' file should be submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T09:48:03.673462Z",
     "iopub.status.idle": "2023-11-13T09:48:03.673837Z",
     "shell.execute_reply": "2023-11-13T09:48:03.673681Z",
     "shell.execute_reply.started": "2023-11-13T09:48:03.673667Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_batch_size = 200\n",
    "\n",
    "# build testing dataloader\n",
    "private_test = np.load(data_root + 'privatetestingset.npy', allow_pickle=True)\n",
    "private_test_dataset = CustomTensorDataset(private_test)\n",
    "private_test_dataloader = DataLoader(private_test_dataset, shuffle=False, batch_size=batch_size)\n",
    "eval_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "# load trained model\n",
    "layer_state_dict = paddle.load('best_model_{}.pdparams'.format(model_type))\n",
    "model.set_state_dict(layer_state_dict)\n",
    "model.eval()\n",
    "\n",
    "# prediction file \n",
    "out_file = 'a3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T09:48:03.675565Z",
     "iopub.status.idle": "2023-11-13T09:48:03.676040Z",
     "shell.execute_reply": "2023-11-13T09:48:03.675894Z",
     "shell.execute_reply.started": "2023-11-13T09:48:03.675869Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomality = list()\n",
    "for i, data in enumerate(private_test_dataloader()): \n",
    "    img = data[0]\n",
    "    if model_type == 'fcn':\n",
    "        img = paddle.reshape(img, [img.shape[0], -1])\n",
    "    output = model(img)\n",
    "    if model_type == 'vae':\n",
    "        output = output[0]\n",
    "    if model_type =='fcn':\n",
    "        loss = eval_loss(output, img).sum(-1)\n",
    "    else:\n",
    "        loss = eval_loss(output, img).sum([1,2,3])\n",
    "    anomality.append(loss.numpy())\n",
    "anomality = np.concatenate(anomality, axis=0)\n",
    "anomality = np.sqrt(anomality).reshape(len(test),1)\n",
    "\n",
    "df = pd.DataFrame(anomality, columns=['score'])\n",
    "df.to_csv(out_file, index_label = 'ID')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
